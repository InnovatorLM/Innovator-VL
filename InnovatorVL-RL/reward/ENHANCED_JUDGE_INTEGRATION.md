# å¢å¼ºç‰ˆJudgeæ¨¡å‹é›†æˆæŒ‡å—

æœ¬æ–‡æ¡£è¯´æ˜å¦‚ä½•å°†æ–°çš„å¢å¼ºç‰ˆJudgeæ¨¡å‹é›†æˆåˆ°LLaVA-OneVision-1.5çš„è®­ç»ƒç³»ç»Ÿä¸­ã€‚

## ğŸš€ æ–°ç‰¹æ€§

### 1. **æ™ºèƒ½å¤šå±‚è¯„åˆ†**
- âœ… **æ ¼å¼åˆ† (5-10%)**ï¼šé¼“åŠ±æ ‡å‡†çš„å›ç­”æ ¼å¼
- âœ… **æ€è€ƒåˆ† (15-35%)**ï¼šè¯„ä¼°æ¨ç†è¿‡ç¨‹çš„è´¨é‡
- âœ… **ç­”æ¡ˆåˆ† (60-80%)**ï¼šæ ¸å¿ƒæ­£ç¡®æ€§éªŒè¯

### 2. **çµæ´»çš„ç­”æ¡ˆæå–**
æ”¯æŒ10+ç§ç­”æ¡ˆæ ¼å¼ï¼š
```
<answer>4</answer>           # æ¨èæ ¼å¼
\boxed{4}                    # LaTeXæ ¼å¼
ç­”æ¡ˆæ˜¯4                     # è‡ªç„¶è¯­è¨€
(A)                         # é€‰æ‹©é¢˜
(1,2,3,4)                   # å…ƒç»„æ ¼å¼
```

### 3. **promptç±»å‹è¯†åˆ«**
è‡ªåŠ¨æ ¹æ®system promptç±»å‹è°ƒæ•´è¯„åˆ†æƒé‡ï¼š
- **thinking prompt**ï¼šé‡è§†æ€è€ƒè¿‡ç¨‹ (æ€è€ƒ30%, ç­”æ¡ˆ60%, æ ¼å¼10%)
- **normal prompt**ï¼šé‡è§†ç­”æ¡ˆæ­£ç¡®æ€§ (æ€è€ƒ15%, ç­”æ¡ˆ80%, æ ¼å¼5%)

### 4. **å¤šå±‚éªŒè¯æœºåˆ¶**
```
ç­”æ¡ˆéªŒè¯é“¾ï¼š
â”œâ”€ Level 1: ç²¾ç¡®å­—ç¬¦ä¸²åŒ¹é…
â”œâ”€ Level 2: æ•°å­¦è¡¨è¾¾å¼ç­‰ä»·éªŒè¯
â”œâ”€ Level 3: é€‰æ‹©é¢˜æ ‡å‡†åŒ–åŒ¹é…
â”œâ”€ Level 4: è¯­ä¹‰åŒ…å«éªŒè¯
â””â”€ Level 5: æ•°å€¼æå–éªŒè¯
```

## ğŸ“¦ é›†æˆæ­¥éª¤

### Step 1: æ›´æ–°é…ç½®æ–‡ä»¶

åœ¨è®­ç»ƒYAMLé…ç½®ä¸­æ·»åŠ ï¼š
```yaml
# configs/llavaov15-8b_stage2_grpo.yaml

reward:
  type: enhanced_judge
  config:
    # åŸºæœ¬é…ç½®
    async_pool_size: 4
    timeout: 30.0

    # è¯„åˆ†æƒé‡ï¼ˆæ ¹æ®promptç±»å‹ï¼‰
    scoring_weights:
      thinking_prompt:
        format: 0.10      # æ ¼å¼åˆ†
        thinking: 0.30    # æ€è€ƒè´¨é‡
        answer: 0.60      # ç­”æ¡ˆæ­£ç¡®æ€§
      normal_prompt:
        format: 0.05
        thinking: 0.15
        answer: 0.80

    # éªŒè¯å±‚çº§é…ç½®
    validation_layers:
      - exact_match
      - math_verify
      - semantic
      - numerical
```

### Step 2: ä¿®æ”¹è®­ç»ƒè„šæœ¬

åœ¨`trains/grpo.py`ä¸­æ›´æ–°ï¼š
```python
# æ›¿æ¢åŸæœ‰çš„import
# from reward.reward_system import RewardSystem
from reward.enhanced_judge_adapter import create_enhanced_judge_reward

# åˆå§‹åŒ–å¥–åŠ±ç³»ç»Ÿ
reward_config = config.get('reward', {}).get('config', {})
reward_system = create_enhanced_judge_reward(
    config=reward_config,
    use_enhanced=config.get('reward', {}).get('type') == 'enhanced_judge'
)

# å¯åŠ¨å‰åˆå§‹åŒ–
reward_system.start()
```

### Step 3: ç¡®ä¿æ•°æ®ä¼ é€’promptç±»å‹

ä¿®æ”¹æ•°æ®åŠ è½½é€»è¾‘ä»¥ä¼ é€’prompt_typeï¼š
```python
# åœ¨å¥–åŠ±è®¡ç®—éƒ¨åˆ†
result = reward_system.reward(
    prompt=batch.get("prompt", ""),
    completion=response,
    answer=ground_truth,
    prompt_type=batch.get("prompt_type", "normal"),  # æ–°å¢
    answer_type=batch.get("answer_type", "ANY")      # æ–°å¢
)
```

### Step 4: ç¯å¢ƒå˜é‡é…ç½®

```bash
# å¯é€‰çš„ç¯å¢ƒå˜é‡
export JUDGE_ASYNC_POOL_SIZE=4
export JUDGE_TIMEOUT=30
export ENABLE_ENHANCED_JUDGE=true
```

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### ä»£ç ä¸­ä½¿ç”¨
```python
from reward.enhanced_judge_adapter import create_enhanced_judge_reward

# åˆ›å»ºå¥–åŠ±ç³»ç»Ÿ
reward = create_enhanced_judge_reward({
    "scoring_weights": {
        "thinking": 0.30,
        "answer": 0.60,
        "format": 0.10
    }
})
reward.start()

# è¯„ä¼°å›å¤
result = reward.reward(
    prompt="è®¡ç®—2+2",
    completion="""
<think>
è®©æˆ‘è®¡ç®—2+2ï¼š
ç¬¬ä¸€æ­¥ï¼šçœ‹åˆ°ä¸¤ä¸ªæ•°2å’Œ2
ç¬¬äºŒæ­¥ï¼šæ‰§è¡ŒåŠ æ³•è¿ç®—
ç¬¬ä¸‰æ­¥ï¼šå¾—åˆ°ç»“æœ4
</think>
<answer>4</answer>
    "",
    answer="4",
    prompt_type="thinking",
    answer_type="NUMBER"
)

print(f"æ€»åˆ†: {result['reward']:.3f}")
print(f"ç­”æ¡ˆåˆ†: {result['acc_reward']:.3f}")
print(f"æ€è€ƒåˆ†: {result['thinking_reward']:.3f}")
print(f"æ ¼å¼åˆ†: {result['format_score']:.3f}")
```

### ç›‘æ§è®­ç»ƒè¿‡ç¨‹
åœ¨è®­ç»ƒæ—¥å¿—ä¸­æŸ¥çœ‹è¯¦ç»†çš„è¯„åˆ†ä¿¡æ¯ï¼š
```
[Step 100] Reward: 0.850 (æ ¼å¼:1.0, æ€è€ƒ:0.8, ç­”æ¡ˆ:1.0, æ–¹æ³•:exact_match)
[Step 200] Reward: 0.675 (æ ¼å¼:1.0, æ€è€ƒ:0.5, ç­”æ¡ˆ:0.9, æ–¹æ³•:math_verify)
[Step 300] Reward: 0.420 (æ ¼å¼:0.0, æ€è€ƒ:0.4, ç­”æ¡ˆ:0.8, æ–¹æ³•:contained)
```

## ğŸ” é«˜çº§é…ç½®

### 1. è‡ªå®šä¹‰éªŒè¯å±‚
```python
validation_layers = [
    "exact_match",      # ç²¾ç¡®åŒ¹é…
    "math_verify",      # æ•°å­¦è¡¨è¾¾å¼éªŒè¯
    "choice_normalize", # é€‰æ‹©é¢˜æ ‡å‡†åŒ–
    "semantic_similar", # è¯­ä¹‰ç›¸ä¼¼åº¦
    "numerical_approx", # æ•°å€¼è¿‘ä¼¼åŒ¹é…
]
```

### 2. æ€è€ƒè´¨é‡è¯„ä¼°å‚æ•°
```python
thinking_evaluation = {
    "min_length": 20,
    "max_length": 200,
    "logic_signals": ["é¦–å…ˆ", "ç„¶å", "æ‰€ä»¥", "step 1", "finally"],
    "math_signals": ["è®¡ç®—", "æ¨å¯¼", "æ£€æŸ¥", "because", "therefore"],
    "relevance_threshold": 0.3
}
```

### 3. ä¸åŒä»»åŠ¡ç±»å‹çš„æƒé‡
```python
# æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒæ•´æƒé‡
task_weights = {
    "multiple_choice": {
        "format": 0.05,
        "thinking": 0.15,
        "answer": 0.80
    },
    "math_expression": {
        "format": 0.10,
        "thinking": 0.25,
        "answer": 0.65
    },
    "spatial_reasoning": {
        "format": 0.15,      # æ ¼å¼æ›´é‡è¦ï¼ˆåæ ‡ç­‰ï¼‰
        "thinking": 0.35,    # æ¨ç†è¿‡ç¨‹é‡è¦
        "answer": 0.50
    }
}
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ‰¹é‡å¤„ç†è®¾ç½®
```python
config = {
    "async_pool_size": 4,      # å¹¶è¡Œå¤„ç†æ•°é‡
    "batch_size": 32,          # æ‰¹å¤„ç†å¤§å°
    "timeout": 30.0,           # å•ä¸ªè¯·æ±‚è¶…æ—¶
    "max_queue_size": 1000     # æœ€å¤§é˜Ÿåˆ—é•¿åº¦
}
```

### 2. ç¼“å­˜é…ç½®
```python
"use_cache": True,
"cache_size": 5000,     # æœ€å¤§ç¼“å­˜æ¡ç›®
"cache_ttl": 3600,      # ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
"cache_key_fields": ["prompt", "completion", "answer"]
```

### 3. é™çº§æœºåˆ¶
```python
"fallback_to_rule": True,
"fallback_conditions": [
    "timeout",          # è¶…æ—¶æ—¶é™çº§
    "circuit_breaker",  # ç†”æ–­æ—¶é™çº§
    "high_error_rate"   # é”™è¯¯ç‡é«˜æ—¶é™çº§
]
```

## ğŸ¬ æœ€åé›†æˆåˆ°é…ç½®æ–‡ä»¶

å®Œæ•´é…ç½®ç¤ºä¾‹ï¼š

```yaml
# configs/llavaov15-8b_stage2_enhanced.yaml

experiment_name: stage2-enhanced-judge
trial_name: trial1
seed: 42

# ... å…¶ä»–é…ç½® ...

reward:
  type: enhanced_judge  # æ–°å¢å­—æ®µ
  config:
    async_pool_size: 4
    timeout: 30.0
    default_prompt_type: "normal"

    # è¯„åˆ†æƒé‡ - æ ¹æ®promptç±»å‹è‡ªé€‚åº”
    scoring_weights:
      thinking_prompt:
        format: 0.10
        thinking: 0.35
        answer: 0.55
      normal_prompt:
        format: 0.05
        thinking: 0.15
        answer: 0.80

    # éªŒè¯é…ç½®
    validation_layers:
      - exact_match
      - math_verify
      - choice_normalize
      - semantic_similar

    # æ€è€ƒè´¨é‡è¯„ä¼°
    thinking_evaluation:
      min_length: 15
      logic_signals: ["é¦–å…ˆ", "ç„¶å", "æ‰€ä»¥", "step 1", "since", "because"]
      math_signals: ["è®¡ç®—", "æ¨å¯¼", "check", "solve", "compute"]
      relevance_threshold: 0.25

    # æ€§èƒ½ä¼˜åŒ–
    caching:
      enabled: true
      size: 5000
      ttl: 3600

    # é™çº§ç­–ç•¥
    fallback:
      enabled: true
      conditions: ["timeout", "error_rate_high", "judge_unavailable"]

# æ•°æ®é›†é…ç½®
train_dataset:
  batch_size: 32
  path: /mnt/innovator/data/wenzichen/mvp-lab/RL-Data/stage2-long
  # ç¡®ä¿æ•°æ®åŒ…å«prompt_typeå­—æ®µ
  extra_fields: ["prompt_type", "answer_type"]
```

## ğŸ” ç›‘æ§ä¸è°ƒè¯•

### æ£€æŸ¥è¯„åˆ†åˆ†å¸ƒ
```bash
# åœ¨è®­ç»ƒæ—¥å¿—ä¸­æŸ¥çœ‹
jupyter notebook analysis/reward_analysis.ipynb
```

### æ€§èƒ½æŒ‡æ ‡
- å¹³å‡è¯„åˆ†ï¼šåº”è¯¥åœ¨0.3-0.7ä¹‹é—´
- æ€è€ƒåˆ†ï¼šæ€è€ƒæ¨¡å¼åº”è¯¥æ˜¾è‘—é«˜äºæ™®é€šæ¨¡å¼
- éªŒè¯æ–¹æ³•ï¼šç›‘æ§ä½¿ç”¨çš„éªŒè¯å±‚çº§
- é™çº§ç‡ï¼šåº”è¯¥ä½äº5%

### å¸¸è§é—®é¢˜
1. **è¯„åˆ†è¿‡ä½ï¼Ÿ** æ£€æŸ¥æƒé‡åˆ†é…å’ŒéªŒè¯å±‚çº§
2. **æ€è€ƒåˆ†å¤ªä½ï¼Ÿ** è°ƒæ•´thinking_evaluationå‚æ•°
3. **é™çº§é¢‘ç¹ï¼Ÿ** æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œjudgeæ¨¡å‹å“åº”

## âœ… éªŒè¯é›†æˆæˆåŠŸ

è¿è¡Œé›†æˆæµ‹è¯•ï¼š
```python
python -m reward.enhanced_judge_adapter --test
```

è¾“å‡ºåº”è¯¥æ˜¾ç¤ºï¼š
```
âœ“ Enhanced judge system initialized
âœ“ Async pool created (4 workers)
âœ“ Judged 10 test cases successfully
âœ“ Average score: 0.812
âœ“ Thinking vs Normal pattern recognition: OK
```

è¿™æ ·å°±æˆåŠŸé›†æˆäº†å¢å¼ºç‰ˆJudgeæ¨¡å‹ï¼Œå®ƒå°†æä¾›æ›´æ™ºèƒ½ã€æ›´ç»†ç²’åº¦çš„å¥–åŠ±è¯„ä¼°ï¼Œç‰¹åˆ«é€‚åˆéœ€è¦æ¨ç†é“¾çš„å¤šæ¨¡æ€ä»»åŠ¡è®­ç»ƒã€‚""""