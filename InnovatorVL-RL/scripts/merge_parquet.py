#!/usr/bin/env python3
"""
åˆå¹¶ç›®å½•ä¸‹æ‰€æœ‰parquetæ–‡ä»¶åˆ°ä¸€ä¸ªparquetæ–‡ä»¶
"""
import os
import glob
import io
from datasets import load_dataset, concatenate_datasets, Dataset, Features, Value
from PIL.Image import Image as ImageObject
from tqdm import tqdm
import pyarrow.parquet as pq
import pyarrow as pa

def merge_parquet_files(input_dir, output_file):
    """
    é€’å½’æŸ¥æ‰¾ç›®å½•ä¸‹æ‰€æœ‰parquetæ–‡ä»¶å¹¶åˆå¹¶
    
    Args:
        input_dir: è¾“å…¥ç›®å½•è·¯å¾„
        output_file: è¾“å‡ºparquetæ–‡ä»¶è·¯å¾„
    """
    # é€’å½’æŸ¥æ‰¾æ‰€æœ‰parquetæ–‡ä»¶
    parquet_files = sorted(glob.glob(os.path.join(input_dir, "**", "*.parquet"), recursive=True))
    
    if not parquet_files:
        print(f"âŒ åœ¨ {input_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½•parquetæ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(parquet_files)} ä¸ªparquetæ–‡ä»¶:")
    for f in parquet_files[:5]:
        print(f"   - {f}")
    if len(parquet_files) > 5:
        print(f"   ... è¿˜æœ‰ {len(parquet_files) - 5} ä¸ªæ–‡ä»¶")
    
    # åŠ è½½æ‰€æœ‰æ•°æ®é›†
    print(f"\nğŸ“¥ æ­£åœ¨åŠ è½½æ•°æ®é›†...")
    dataset_list = []
    for parquet_file in tqdm(parquet_files, desc="Loading"):
        try:
            # é¦–å…ˆå°è¯•ä½¿ç”¨ datasets åº“åŠ è½½
            try:
                dataset = load_dataset("parquet", data_files=parquet_file)['train']
            except (Exception, TypeError, ValueError) as e1:
                # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨ pyarrow è¯»å–å¹¶æ‰‹åŠ¨å¤„ç†
                print(f"   âš ï¸  datasets åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨ pyarrow æ‰‹åŠ¨å¤„ç†: {str(e1)[:100]}")
                try:
                    # ä½¿ç”¨ pyarrow è¯»å–ï¼ˆå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç† PIL Imageï¼‰
                    # å°è¯•ç›´æ¥è¯»å–ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨å…¶ä»–æ–¹æ³•
                    try:
                        table = pq.read_table(parquet_file)
                    except Exception as read_err:
                        print(f"   âš ï¸  pyarrow è¯»å–å¤±è´¥: {read_err}")
                        raise e1  # é‡æ–°æŠ›å‡ºåŸå§‹é”™è¯¯
                    
                    # è¯»å–æ‰€æœ‰æ•°æ®ä¸º Python åˆ—è¡¨
                    try:
                        data = table.to_pylist()
                    except Exception as pylist_err:
                        print(f"   âš ï¸  è½¬æ¢ä¸º Python åˆ—è¡¨å¤±è´¥: {pylist_err}")
                        # å°è¯•é€è¡Œè¯»å–
                        data = []
                        for batch in table.to_batches():
                            data.extend(batch.to_pylist())
                    
                    # è½¬æ¢ images å­—æ®µï¼šå°† PIL Image è½¬æ¢ä¸º bytes æ ¼å¼
                    def convert_image_to_bytes(img):
                        """å°†å•ä¸ªå›¾åƒè½¬æ¢ä¸º bytes æ ¼å¼"""
                        if isinstance(img, ImageObject):
                            img_bytes = io.BytesIO()
                            if img.mode != "RGB":
                                img = img.convert("RGB")
                            img.save(img_bytes, format="PNG")
                            return {"bytes": img_bytes.getvalue(), "path": None}
                        elif isinstance(img, dict):
                            # å·²ç»æ˜¯å­—å…¸æ ¼å¼
                            return img
                        elif img is None:
                            return None
                        elif hasattr(img, '__class__') and 'Image' in str(type(img)):
                            # å¯èƒ½æ˜¯å…¶ä»– Image ç±»å‹ï¼Œå°è¯•è½¬æ¢
                            try:
                                img_bytes = io.BytesIO()
                                if hasattr(img, 'mode') and img.mode != "RGB":
                                    img = img.convert("RGB")
                                img.save(img_bytes, format="PNG")
                                return {"bytes": img_bytes.getvalue(), "path": None}
                            except Exception as conv_err:
                                print(f"   âš ï¸  å›¾åƒè½¬æ¢å¤±è´¥: {conv_err}")
                                return {"bytes": None, "path": None}
                        else:
                            # æœªçŸ¥æ ¼å¼ï¼Œå°è¯•æ£€æŸ¥
                            return {"bytes": None, "path": None}
                    
                    # å¤„ç†æ¯ä¸ªæ ·æœ¬çš„ images å­—æ®µ
                    if "images" in table.column_names:
                        for i, sample in enumerate(data):
                            try:
                                if "images" in sample and sample["images"] is not None:
                                    if isinstance(sample["images"], list):
                                        sample["images"] = [convert_image_to_bytes(img) for img in sample["images"]]
                                    else:
                                        sample["images"] = convert_image_to_bytes(sample["images"])
                            except Exception as img_err:
                                print(f"   âš ï¸  å¤„ç†æ ·æœ¬ {i} çš„ images æ—¶å‡ºé”™: {img_err}")
                                # ç»§ç»­å¤„ç†å…¶ä»–æ ·æœ¬
                    
                    # ä½¿ç”¨ Dataset.from_dict åˆ›å»º Dataset
                    # Dataset.from_dict éœ€è¦ {column_name: [values]} æ ¼å¼
                    if not data:
                        raise ValueError("æ•°æ®ä¸ºç©ºï¼Œæ— æ³•åˆ›å»º Dataset")
                    
                    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼š{column: [æ‰€æœ‰è¡Œçš„è¯¥åˆ—å€¼]}
                    columns = list(data[0].keys())
                    dataset_dict = {}
                    for col in columns:
                        dataset_dict[col] = [row[col] for row in data]
                    
                    # åˆ›å»º Dataset
                    dataset = Dataset.from_dict(dataset_dict)
                    
                    # ä½¿ç”¨ map æ ‡å‡†åŒ– images æ ¼å¼ï¼ˆå‚è€ƒ clean_data.py çš„åšæ³•ï¼‰
                    # ç¡®ä¿ images å­—æ®µç»Ÿä¸€ä¸º {bytes, path} æ ¼å¼
                    if "images" in dataset.column_names:
                        def standardize_row(example):
                            """æ ‡å‡†åŒ–æ•°æ®è¡Œæ ¼å¼ï¼Œç‰¹åˆ«æ˜¯ images å­—æ®µ"""
                            # å¤„ç† images å­—æ®µ
                            if "images" in example and example["images"] is not None:
                                standardized_images = []
                                for img in example["images"]:
                                    if isinstance(img, dict) and "bytes" in img:
                                        # å·²ç»æ˜¯æ ‡å‡†æ ¼å¼
                                        standardized_images.append({
                                            "bytes": img.get("bytes"),
                                            "path": str(img.get("path")) if img.get("path") is not None else None
                                        })
                                    elif isinstance(img, ImageObject):
                                        # PIL Image -> bytes
                                        img_bytes = io.BytesIO()
                                        if img.mode != "RGB":
                                            img = img.convert("RGB")
                                        img.save(img_bytes, format="PNG")
                                        standardized_images.append({
                                            "bytes": img_bytes.getvalue(),
                                            "path": None
                                        })
                                example["images"] = standardized_images
                            return example
                        
                        # ä½¿ç”¨ map æ ‡å‡†åŒ–æ ¼å¼ï¼ˆä¸æŒ‡å®š featuresï¼Œè®© datasets è‡ªåŠ¨æ¨æ–­ï¼‰
                        dataset = dataset.map(
                            standardize_row,
                            desc=f"Standardizing {os.path.basename(parquet_file)}",
                            num_proc=1
                        )
                except Exception as e2:
                    print(f"   âš ï¸  pyarrow å¤„ç†ä¹Ÿå¤±è´¥: {e2}")
                    import traceback
                    traceback.print_exc()
                    continue
            
            # æ£€æŸ¥å¹¶è½¬æ¢ images å­—æ®µï¼šå°† PIL Image è½¬æ¢ä¸º bytes æ ¼å¼ï¼ˆåŒé‡ä¿é™©ï¼‰
            if "images" in dataset.column_names:
                def convert_images_to_bytes(example):
                    """å°† PIL Image è½¬æ¢ä¸º bytes æ ¼å¼çš„å­—å…¸"""
                    if "images" in example and example["images"] is not None:
                        converted_images = []
                        for img in example["images"]:
                            if isinstance(img, ImageObject):
                                # PIL Image -> bytes
                                img_bytes = io.BytesIO()
                                # ç¡®ä¿æ˜¯ RGB æ¨¡å¼
                                if img.mode != "RGB":
                                    img = img.convert("RGB")
                                img.save(img_bytes, format="PNG")
                                converted_images.append({"bytes": img_bytes.getvalue(), "path": None})
                            elif isinstance(img, dict):
                                # å·²ç»æ˜¯å­—å…¸æ ¼å¼ï¼Œä¿æŒä¸å˜
                                converted_images.append(img)
                            elif img is None:
                                # None å€¼ï¼Œè·³è¿‡
                                continue
                            else:
                                # å…¶ä»–æ ¼å¼
                                converted_images.append({"bytes": None, "path": None})
                        example["images"] = converted_images
                    return example
                
                # è½¬æ¢å›¾åƒæ ¼å¼
                dataset = dataset.map(convert_images_to_bytes, desc=f"Converting images in {os.path.basename(parquet_file)}")
            
            dataset_list.append(dataset)
        except Exception as e:
            print(f"âš ï¸  åŠ è½½ {parquet_file} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    if not dataset_list:
        print("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®é›†")
        return
    
    print(f"\nâœ… æˆåŠŸåŠ è½½ {len(dataset_list)} ä¸ªæ•°æ®é›†")
    
    # å¯¹é½åˆ—ï¼ˆå¤„ç†å¯èƒ½çš„åˆ—ä¸ä¸€è‡´é—®é¢˜ï¼‰
    print(f"\nğŸ”§ å¯¹é½åˆ—ç»“æ„...")
    all_features = set()
    for ds in dataset_list:
        all_features.update(ds.column_names)
    
    print(f"   å‘ç° {len(all_features)} ä¸ªä¸åŒçš„åˆ—: {sorted(all_features)}")
    
    def add_missing_columns(example, current_columns):
        for feature in all_features:
            if feature not in current_columns:
                example[feature] = None
        return example
    
    dataset_list = [
        ds.map(lambda x: add_missing_columns(x, ds.column_names), desc="Aligning columns") 
        if set(ds.column_names) != all_features else ds 
        for ds in tqdm(dataset_list, desc="Aligning")
    ]
    
    # ç»Ÿä¸€ç‰¹å¾ç±»å‹ï¼ˆå¤„ç†ç±»å‹ä¸åŒ¹é…é—®é¢˜ï¼‰
    if dataset_list:
        target_features = dataset_list[0].features
        new_list = []
        for ds in dataset_list:
            try:
                ds = ds.cast(target_features)
            except Exception as e:
                print(f"âš ï¸  è½¬æ¢ç‰¹å¾ç±»å‹æ—¶å‡ºé”™ï¼ˆç»§ç»­å¤„ç†ï¼‰: {e}")
            new_list.append(ds)
        dataset_list = new_list
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®é›†
    print(f"\nğŸ”€ åˆå¹¶æ•°æ®é›†...")
    merged_dataset = concatenate_datasets(dataset_list)
    
    print(f"\nğŸ“Š åˆå¹¶åçš„æ•°æ®é›†ç»Ÿè®¡:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(merged_dataset)}")
    print(f"   åˆ—æ•°: {len(merged_dataset.column_names)}")
    print(f"   åˆ—å: {merged_dataset.column_names}")
    
    # ä¿å­˜åˆ°parquetæ–‡ä»¶
    print(f"\nğŸ’¾ ä¿å­˜åˆ° {output_file}...")
    merged_dataset.to_parquet(output_file)
    
    print(f"\nâœ… å®Œæˆ! åˆå¹¶åçš„æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
    print(f"   æ–‡ä»¶å¤§å°: {os.path.getsize(output_file) / (1024**3):.2f} GB")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 3:
        print("ç”¨æ³•: python merge_parquet.py <è¾“å…¥ç›®å½•> <è¾“å‡ºæ–‡ä»¶è·¯å¾„>")
        print("ç¤ºä¾‹: python merge_parquet.py /path/to/input /path/to/output.parquet")
        sys.exit(1)
    
    input_dir = sys.argv[1]
    output_file = sys.argv[2]
    
    if not os.path.isdir(input_dir):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        sys.exit(1)
    
    # å¦‚æœè¾“å‡ºè·¯å¾„æ²¡æœ‰ .parquet æ‰©å±•åï¼Œè‡ªåŠ¨æ·»åŠ 
    if not output_file.endswith('.parquet'):
        output_file = output_file + '.parquet'
        print(f"ğŸ“ è‡ªåŠ¨æ·»åŠ  .parquet æ‰©å±•å: {output_file}")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(output_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
        print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    merge_parquet_files(input_dir, output_file)

