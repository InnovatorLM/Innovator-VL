#!/usr/bin/env python3
"""
ç»Ÿä¸€æ•°æ®é›†æ ¼å¼è„šæœ¬
å°†æ‰€æœ‰æ•°æ®é›†æ–‡ä»¶ç»Ÿä¸€ä¸ºç›¸åŒçš„æ ¼å¼ï¼Œç¡®ä¿æ¯ä¸ªå­—æ®µçš„ç±»å‹éƒ½ä¸€è‡´
è¿™æ ·å¯ä»¥é¿å…åœ¨ concatenate_datasets æ—¶å‡ºç°ç±»å‹ä¸åŒ¹é…çš„é—®é¢˜
"""

import sys
import argparse
import os
import glob
from pathlib import Path
from typing import Dict, Any, Set, List
from collections import defaultdict

from datasets import load_dataset, Dataset, Features, Sequence, Image as DatasetImage, Value
from datasets.features.features import List as FeaturesList
from tqdm import tqdm
import io
from PIL import Image

# éœ€è¦ç§»é™¤çš„å­—æ®µåˆ—è¡¨ï¼ˆè®­ç»ƒæ—¶ä¸éœ€è¦çš„å­—æ®µï¼‰
FIELDS_TO_REMOVE = [
    "reward_model",
    "vanilla_prompt",
]

# æ ‡å‡†å­—æ®µç±»å‹å®šä¹‰
STANDARD_FEATURES = {
    "id": Value("string"),
    "problem": Value("string"),
    "answer": FeaturesList(Value("string")),
    "images": Sequence(DatasetImage(decode=True)),  # ç»Ÿä¸€ä½¿ç”¨ Image ç±»å‹
    "problem_type": Value("string"),
    "answer_type": Value("string"),
    "source": Value("string"),
    "prompt_type": Value("string"),
    "messages": Value("string"),
    "data_source": Value("string"),  # å…è®¸ null
    "solution": Value("string"),  # å…è®¸ null
    "avg_reward": Value("float"),  # å…è®¸ null
    "options": Value("string"),  # å…è®¸ null
    "tokens": Value("int64"),  # å…è®¸ null
    "ability": Value("string"),  # å…è®¸ null
    "format_guidance": Value("string"),  # å…è®¸ null
}

def analyze_all_datasets(input_paths: List[str]) -> Dict[str, Any]:
    """
    åˆ†ææ‰€æœ‰æ•°æ®é›†ï¼Œæ”¶é›†æ‰€æœ‰å­—æ®µå’Œå®ƒä»¬çš„ç±»å‹
    è¿”å›ï¼šå­—æ®µå -> æ‰€æœ‰å‡ºç°è¿‡çš„ç±»å‹çš„é›†åˆ
    """
    print("=" * 80)
    print("æ­¥éª¤ 1: åˆ†ææ‰€æœ‰æ•°æ®é›†ï¼Œæ”¶é›†å­—æ®µä¿¡æ¯...")
    print("=" * 80)
    
    all_fields: Set[str] = set()
    field_types: Dict[str, Set[str]] = defaultdict(set)
    
    # å±•å¼€æ‰€æœ‰è·¯å¾„
    expanded_paths = []
    for path in input_paths:
        path = path.strip()
        if "*" in path:
            expanded_paths.extend(sorted(glob.glob(path)))
        elif os.path.isdir(path):
            expanded_paths.extend(sorted(glob.glob(os.path.join(path, "**", "*.parquet"), recursive=True)))
        elif path.endswith((".parquet", ".json")):
            expanded_paths.append(path)
    
    print(f"æ‰¾åˆ° {len(expanded_paths)} ä¸ªæ•°æ®é›†æ–‡ä»¶")
    
    for path in tqdm(expanded_paths, desc="åˆ†ææ•°æ®é›†"):
        try:
            if path.endswith(".parquet"):
                dataset = load_dataset("parquet", data_files=path)['train']
            elif path.endswith(".json"):
                dataset = load_dataset("json", data_files=path)['train']
            else:
                continue
            
            # æ”¶é›†å­—æ®µä¿¡æ¯
            for field_name in dataset.column_names:
                if field_name in FIELDS_TO_REMOVE:
                    continue
                all_fields.add(field_name)
                field_type_str = str(dataset.features[field_name])
                field_types[field_name].add(field_type_str)
        except Exception as e:
            print(f"âš ï¸  è·³è¿‡æ–‡ä»¶ {path}: {e}")
            continue
    
    print(f"\nå‘ç° {len(all_fields)} ä¸ªå”¯ä¸€å­—æ®µ:")
    for field in sorted(all_fields):
        types = field_types[field]
        print(f"  - {field}: {len(types)} ç§ç±»å‹")
        if len(types) > 1:
            for t in sorted(types):
                print(f"      {t}")
    
    return {
        "all_fields": all_fields,
        "field_types": field_types,
        "expanded_paths": expanded_paths
    }

def determine_standard_features(all_fields: Set[str], field_types: Dict[str, Set[str]]) -> Features:
    """
    æ ¹æ®åˆ†æç»“æœç¡®å®šæ ‡å‡†ç‰¹å¾ç±»å‹
    """
    print("\n" + "=" * 80)
    print("æ­¥éª¤ 2: ç¡®å®šæ ‡å‡†ç‰¹å¾ç±»å‹...")
    print("=" * 80)
    
    standard_features_dict = {}
    
    for field in sorted(all_fields):
        if field in FIELDS_TO_REMOVE:
            continue
        
        # å¦‚æœå­—æ®µåœ¨æ ‡å‡†å®šä¹‰ä¸­ï¼Œä½¿ç”¨æ ‡å‡†å®šä¹‰
        if field in STANDARD_FEATURES:
            standard_features_dict[field] = STANDARD_FEATURES[field]
            print(f"  âœ… {field}: ä½¿ç”¨é¢„å®šä¹‰æ ‡å‡†ç±»å‹")
        else:
            # æ ¹æ®å‡ºç°çš„ç±»å‹æ¨æ–­æ ‡å‡†ç±»å‹
            types = field_types[field]
            type_strs = list(types)
            
            # ç®€å•çš„ç±»å‹æ¨æ–­é€»è¾‘
            if any("string" in t.lower() for t in type_strs):
                standard_features_dict[field] = Value("string")
                print(f"  ğŸ“ {field}: æ¨æ–­ä¸º Value('string')")
            elif any("int" in t.lower() for t in type_strs):
                standard_features_dict[field] = Value("int64")
                print(f"  ğŸ“ {field}: æ¨æ–­ä¸º Value('int64')")
            elif any("float" in t.lower() for t in type_strs):
                standard_features_dict[field] = Value("float")
                print(f"  ğŸ“ {field}: æ¨æ–­ä¸º Value('float')")
            elif any("bool" in t.lower() for t in type_strs):
                standard_features_dict[field] = Value("bool")
                print(f"  ğŸ“ {field}: æ¨æ–­ä¸º Value('bool')")
            else:
                # é»˜è®¤ä½¿ç”¨ string
                standard_features_dict[field] = Value("string")
                print(f"  âš ï¸  {field}: æ— æ³•æ¨æ–­ï¼Œé»˜è®¤ä½¿ç”¨ Value('string')")
    
    return Features(standard_features_dict)

def convert_images_to_standard_format(images: Any) -> List[Any]:
    """
    å°†å›¾åƒè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼ï¼ˆPIL Image å¯¹è±¡ï¼Œç”¨äº Sequence(Image(...))ï¼‰
    """
    if images is None or len(images) == 0:
        return []
    
    converted = []
    for img in images:
        if isinstance(img, Image.Image):
            # å·²ç»æ˜¯ PIL Imageï¼Œç›´æ¥ä½¿ç”¨
            converted.append(img)
        elif isinstance(img, dict):
            # ä» dict æ ¼å¼åŠ è½½
            if "bytes" in img and img["bytes"] is not None:
                try:
                    pil_img = Image.open(io.BytesIO(img["bytes"]))
                    if pil_img.mode in ("CMYK", "YCbCr", "LAB", "HSV", "P"):
                        pil_img = pil_img.convert("RGB")
                    converted.append(pil_img)
                except Exception as e:
                    print(f"âš ï¸  æ— æ³•åŠ è½½å›¾åƒ bytes: {e}")
                    continue
            elif "path" in img and img["path"] is not None:
                try:
                    pil_img = Image.open(img["path"])
                    if pil_img.mode in ("CMYK", "YCbCr", "LAB", "HSV", "P"):
                        pil_img = pil_img.convert("RGB")
                    converted.append(pil_img)
                except Exception as e:
                    print(f"âš ï¸  æ— æ³•åŠ è½½å›¾åƒè·¯å¾„ {img['path']}: {e}")
                    continue
        elif isinstance(img, str):
            # æ–‡ä»¶è·¯å¾„
            try:
                pil_img = Image.open(img)
                if pil_img.mode in ("CMYK", "YCbCr", "LAB", "HSV", "P"):
                    pil_img = pil_img.convert("RGB")
                converted.append(pil_img)
            except Exception as e:
                print(f"âš ï¸  æ— æ³•åŠ è½½å›¾åƒè·¯å¾„ {img}: {e}")
                continue
        else:
            print(f"âš ï¸  æœªçŸ¥çš„å›¾åƒæ ¼å¼: {type(img)}")
    
    return converted

def normalize_sample(sample: Dict[str, Any], standard_features: Features) -> Dict[str, Any]:
    """
    å°†å•ä¸ªæ ·æœ¬è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
    """
    normalized = {}
    
    for field_name, field_feature in standard_features.items():
        if field_name not in sample:
            # å­—æ®µä¸å­˜åœ¨ï¼Œè®¾ç½®ä¸º Noneï¼ˆå¦‚æœå…è®¸ï¼‰æˆ–é»˜è®¤å€¼
            if isinstance(field_feature, Value) and field_feature.dtype == "string":
                normalized[field_name] = None
            else:
                normalized[field_name] = None
        else:
            value = sample[field_name]
            
            # ç‰¹æ®Šå¤„ç† images å­—æ®µ
            if field_name == "images":
                normalized[field_name] = convert_images_to_standard_format(value)
            # ç‰¹æ®Šå¤„ç†å…¶ä»–å­—æ®µçš„ç±»å‹è½¬æ¢
            elif isinstance(field_feature, Value):
                # ç®€å•ç±»å‹è½¬æ¢
                if value is None:
                    normalized[field_name] = None
                elif field_feature.dtype == "string":
                    normalized[field_name] = str(value) if value is not None else None
                elif field_feature.dtype == "int64":
                    try:
                        normalized[field_name] = int(value) if value is not None else None
                    except (ValueError, TypeError):
                        normalized[field_name] = None
                elif field_feature.dtype == "float":
                    try:
                        normalized[field_name] = float(value) if value is not None else None
                    except (ValueError, TypeError):
                        normalized[field_name] = None
                else:
                    normalized[field_name] = value
            elif isinstance(field_feature, FeaturesList):
                # List ç±»å‹
                if value is None:
                    normalized[field_name] = []
                elif isinstance(value, list):
                    # è½¬æ¢åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ 
                    inner_feature = field_feature.feature
                    if isinstance(inner_feature, Value):
                        if inner_feature.dtype == "string":
                            normalized[field_name] = [str(v) if v is not None else "" for v in value]
                        else:
                            normalized[field_name] = value
                    else:
                        normalized[field_name] = value
                else:
                    normalized[field_name] = [value] if value is not None else []
            else:
                # å…¶ä»–ç±»å‹ï¼Œä¿æŒåŸæ ·
                normalized[field_name] = value
    
    return normalized

def process_dataset_file(input_path: str, standard_features: Features, output_path: str = None) -> Dataset:
    """
    å¤„ç†å•ä¸ªæ•°æ®é›†æ–‡ä»¶ï¼Œè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
    """
    print(f"\nå¤„ç†æ–‡ä»¶: {input_path}")
    
    # åŠ è½½æ•°æ®é›†
    if input_path.endswith(".parquet"):
        dataset = load_dataset("parquet", data_files=input_path)['train']
    elif input_path.endswith(".json"):
        dataset = load_dataset("json", data_files=input_path)['train']
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {input_path}")
    
    # ç§»é™¤ä¸éœ€è¦çš„å­—æ®µ
    columns_to_keep = [col for col in dataset.column_names if col not in FIELDS_TO_REMOVE]
    if len(columns_to_keep) < len(dataset.column_names):
        dataset = dataset.select_columns(columns_to_keep)
        print(f"  ç§»é™¤äº† {len(dataset.column_names) - len(columns_to_keep)} ä¸ªä¸éœ€è¦çš„å­—æ®µ")
    
    # è½¬æ¢æ¯ä¸ªæ ·æœ¬
    print(f"  è½¬æ¢ {len(dataset)} ä¸ªæ ·æœ¬...")
    
    def normalize_batch(examples):
        """æ‰¹é‡å½’ä¸€åŒ–"""
        batch_size = len(examples[list(examples.keys())[0]])
        normalized_batch = {field: [] for field in standard_features.keys()}
        
        for i in range(batch_size):
            sample = {key: examples[key][i] for key in examples.keys()}
            normalized = normalize_sample(sample, standard_features)
            for field in standard_features.keys():
                normalized_batch[field].append(normalized.get(field))
        
        return normalized_batch
    
    # ç¡®å®šè¦ç§»é™¤çš„åˆ—ï¼ˆä¸åœ¨æ ‡å‡†ç‰¹å¾ä¸­çš„åˆ—ï¼‰
    columns_to_remove = [col for col in dataset.column_names if col not in standard_features]
    
    # ä½¿ç”¨ map æ‰¹é‡å¤„ç†
    dataset = dataset.map(
        normalize_batch,
        batched=True,
        batch_size=1000,
        desc="å½’ä¸€åŒ–æ ·æœ¬",
        remove_columns=columns_to_remove if columns_to_remove else None
    )
    
    # è®¾ç½®æ ‡å‡†ç‰¹å¾
    try:
        dataset = dataset.cast(standard_features)
        print(f"  âœ… æˆåŠŸè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼")
    except Exception as e:
        print(f"  âš ï¸  cast å¤±è´¥: {e}")
        print(f"  ä½¿ç”¨ from_dict é‡æ–°åˆ›å»º...")
        # é‡æ–°åˆ›å»ºæ•°æ®é›†
        data_dict = {col: [dataset[j][col] for j in range(len(dataset))] 
                     for col in dataset.column_names}
        dataset = Dataset.from_dict(data_dict, features=standard_features)
        print(f"  âœ… é€šè¿‡ from_dict æˆåŠŸåˆ›å»ºæ ‡å‡†æ ¼å¼æ•°æ®é›†")
    
    # ä¿å­˜ï¼ˆå¦‚æœæŒ‡å®šäº†è¾“å‡ºè·¯å¾„ï¼‰
    if output_path:
        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else ".", exist_ok=True)
        dataset.to_parquet(output_path)
        print(f"  ğŸ’¾ å·²ä¿å­˜åˆ°: {output_path}")
    
    return dataset

def main():
    parser = argparse.ArgumentParser(description="ç»Ÿä¸€æ•°æ®é›†æ ¼å¼")
    parser.add_argument("input_paths", nargs="+", help="è¾“å…¥æ•°æ®é›†è·¯å¾„ï¼ˆæ”¯æŒ glob æ¨¡å¼ï¼‰")
    parser.add_argument("--output_dir", type=str, help="è¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œåˆ™è¦†ç›–åŸæ–‡ä»¶ï¼‰")
    parser.add_argument("--output_suffix", type=str, default="_unified", help="è¾“å‡ºæ–‡ä»¶åç¼€")
    parser.add_argument("--dry-run", action="store_true", help="åªåˆ†æï¼Œä¸å®é™…è½¬æ¢")
    
    args = parser.parse_args()
    
    # æ­¥éª¤ 1: åˆ†ææ‰€æœ‰æ•°æ®é›†
    analysis_result = analyze_all_datasets(args.input_paths)
    all_fields = analysis_result["all_fields"]
    field_types = analysis_result["field_types"]
    expanded_paths = analysis_result["expanded_paths"]
    
    if args.dry_run:
        print("\nğŸ” è¿™æ˜¯ dry-run æ¨¡å¼ï¼Œä¸ä¼šå®é™…è½¬æ¢æ–‡ä»¶")
        return
    
    # æ­¥éª¤ 2: ç¡®å®šæ ‡å‡†ç‰¹å¾
    standard_features = determine_standard_features(all_fields, field_types)
    
    # æ­¥éª¤ 3: å¤„ç†æ¯ä¸ªæ–‡ä»¶
    print("\n" + "=" * 80)
    print("æ­¥éª¤ 3: è½¬æ¢æ‰€æœ‰æ•°æ®é›†æ–‡ä»¶...")
    print("=" * 80)
    
    for input_path in tqdm(expanded_paths, desc="å¤„ç†æ–‡ä»¶"):
        try:
            if args.output_dir:
                # ä¿å­˜åˆ°æŒ‡å®šç›®å½•
                input_name = Path(input_path).stem
                output_path = os.path.join(args.output_dir, f"{input_name}{args.output_suffix}.parquet")
            else:
                # è¦†ç›–åŸæ–‡ä»¶ï¼ˆæ·»åŠ åç¼€ï¼‰
                output_path = str(Path(input_path).with_suffix("")) + args.output_suffix + ".parquet"
            
            process_dataset_file(input_path, standard_features, output_path)
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶ {input_path} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print("\n" + "=" * 80)
    print("âœ… æ‰€æœ‰æ•°æ®é›†å·²ç»Ÿä¸€æ ¼å¼ï¼")
    print("=" * 80)

if __name__ == "__main__":
    main()

