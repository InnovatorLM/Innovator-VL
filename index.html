<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="Innovator-VL: A Multimodal Large Language Model for Scientific Discovery">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="We present Innovator-VL, a scientific multimodal large language model (MLLM) designed to advance multimodal understanding and reasoning across diverse scientific domains while still maintaining excellent performance on general vision tasks. Contrary to the prevailing trend of relying on massive domain-specific pretraining data and opaque training pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific multimodal intelligence with substantially reduced data requirements.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="MLLMs, science, reinforcement learning">
  <!-- TODO: List all authors -->
  <meta name="author" content="Zichen Wen*†, Boxue Yang*, Shuang Chen, Yaojie Zhang, Yuhang Han, Junlong Ke, Linfeng Zhang‡">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="Innovator">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="Innovator-VL: A Multimodal Large Language Model for Scientific Discovery">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="We present Innovator-VL, a scientific multimodal large language model (MLLM) designed to advance multimodal understanding and reasoning across diverse scientific domains while still maintaining excellent performance on general vision tasks. Contrary to the prevailing trend of relying on massive domain-specific pretraining data and opaque training pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific multimodal intelligence with substantially reduced data requirements.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://innovator-vl.github.io/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="Innovator-VL: A Multimodal Large Language Model for Scientific Discovery - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="FIRST_AUTHOR_NAME">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="Innovator-VL: A Multimodal Large Language Model for Scientific Discovery">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="We present Innovator-VL, a scientific multimodal large language model (MLLM) designed to advance multimodal understanding and reasoning across diverse scientific domains while still maintaining excellent performance on general vision tasks.">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="Innovator-VL: A Multimodal Large Language Model for Scientific Discovery - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="PAPER_TITLE">
  <meta name="citation_author" content="FIRST_AUTHOR_LAST, FIRST_AUTHOR_FIRST">
  <meta name="citation_author" content="SECOND_AUTHOR_LAST, SECOND_AUTHOR_FIRST">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">

  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">

  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">

  <!-- TODO: Replace with your paper title and authors -->
  <title>Innovator-VL: A Multimodal Large Language Model for Scientific Discovery</title>

  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">

  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">

  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>

  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>

  <!-- Charts (for the new Scientific Data section) -->
  <script defer src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>

  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>

  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>

  <!-- ===================== -->
  <!-- Academic Visual Upgrade -->
  <!-- (No changes to existing content; only styling + new section below) -->
  <!-- ===================== -->
  <style>
    :root{
      --ink:#0b1220;
      --muted:#5b6476;
      --paper:#ffffff;
      --paper-2:#f6f8fc;
      --line:rgba(15,23,42,.10);
      --shadow:0 10px 30px rgba(2,6,23,.08);
      --shadow-2:0 14px 50px rgba(2,6,23,.10);
      --radius:18px;
      --radius-2:24px;
      --accent:#2563eb;
      --accent-2:#7c3aed;
      --accent-3:#06b6d4;
    }

    html, body{
      font-family: Inter, system-ui, -apple-system, Segoe UI, Roboto, "Helvetica Neue", Arial, "Noto Sans", "Liberation Sans", sans-serif !important;
      color: var(--ink);
      background: radial-gradient(1200px 700px at 20% -10%, rgba(37,99,235,.16), transparent 60%),
                  radial-gradient(900px 600px at 90% 0%, rgba(124,58,237,.14), transparent 55%),
                  linear-gradient(180deg, #ffffff 0%, #fbfcff 40%, #f6f8fc 100%);
    }

    /* Subtle academic texture */
    body:before{
      content:"";
      position:fixed;
      inset:0;
      pointer-events:none;
      background-image:
        radial-gradient(rgba(2,6,23,.045) 1px, transparent 1px);
      background-size: 22px 22px;
      opacity:.45;
      mix-blend-mode:multiply;
      z-index:-1;
    }

    /* Improved section rhythm */
    section.section, section.hero{
      position:relative;
    }
    .hero.is-light{
      background: linear-gradient(180deg, rgba(246,248,252,.92), rgba(246,248,252,.62)) !important;
    }

    /* Sticky top nav for academic sites */
    .academic-nav{
      position: sticky;
      top:0;
      z-index: 30;
      backdrop-filter: blur(10px);
      -webkit-backdrop-filter: blur(10px);
      background: rgba(255,255,255,.68);
      border-bottom:1px solid var(--line);
    }
    .academic-nav .navbar{
      background: transparent;
    }
    .academic-nav .navbar-item,
    .academic-nav .navbar-link{
      font-weight: 600;
      color: rgba(11,18,32,.82);
    }
    .academic-nav .navbar-item:hover,
    .academic-nav .navbar-link:hover{
      color: var(--ink);
      background: rgba(37,99,235,.06);
      border-radius: 12px;
    }
    .academic-nav .navbar-brand .navbar-item{
      gap:.6rem;
    }
    .badge{
      display:inline-flex;
      align-items:center;
      gap:.45rem;
      font-size:.78rem;
      font-weight:700;
      color: rgba(11,18,32,.84);
      background: rgba(37,99,235,.08);
      border:1px solid rgba(37,99,235,.14);
      padding:.25rem .6rem;
      border-radius:999px;
      margin-left:.25rem;
      white-space:nowrap;
    }

    /* Hero polish without changing content */
    .publication-title{
      letter-spacing: -0.02em;
      line-height: 1.08;
      text-wrap: balance;
    }
    .publication-authors a{
      font-weight: 600;
    }
    .publication-links .button.is-dark{
      background: linear-gradient(135deg, rgba(11,18,32,.92), rgba(11,18,32,.82)) !important;
      border: 1px solid rgba(255,255,255,.08) !important;
      box-shadow: 0 10px 24px rgba(2,6,23,.14);
      transition: transform .18s ease, box-shadow .18s ease, filter .18s ease;
    }
    .publication-links .button.is-dark:hover{
      transform: translateY(-2px);
      box-shadow: 0 16px 40px rgba(2,6,23,.18);
      filter: saturate(1.08);
    }

    /* Figure cards */
    .figure-card{
      background: rgba(255,255,255,.88);
      border: 1px solid var(--line);
      border-radius: var(--radius-2);
      box-shadow: var(--shadow);
      padding: 1.15rem 1.15rem 1rem 1.15rem;
    }
    .figure-card img{
      border-radius: 16px;
      border: 1px solid rgba(15,23,42,.08);
      box-shadow: 0 12px 36px rgba(2,6,23,.10);
    }
    .figure-card .has-text-grey{
      color: rgba(91,100,118,.9) !important;
    }

    /* Academic list styling */
    .content ul{
      margin-top: .75rem;
    }
    .content ul li{
      margin-bottom: .55rem;
    }

    /* Scroll-to-top */
    .scroll-to-top{
      position: fixed;
      right: 18px;
      bottom: 18px;
      width: 44px;
      height: 44px;
      border-radius: 14px;
      border: 1px solid rgba(15,23,42,.12);
      background: rgba(255,255,255,.78);
      box-shadow: 0 12px 28px rgba(2,6,23,.12);
      color: rgba(11,18,32,.84);
      display:none;
      align-items:center;
      justify-content:center;
      cursor:pointer;
      transition: transform .18s ease, opacity .18s ease, background .18s ease;
      backdrop-filter: blur(10px);
      -webkit-backdrop-filter: blur(10px);
    }
    .scroll-to-top:hover{
      transform: translateY(-2px);
      background: rgba(255,255,255,.92);
    }

    /* BibTeX copy button polish (keeps your structure) */
    .copy-bibtex-btn{
      border-radius: 999px;
      border: 1px solid rgba(15,23,42,.12);
      background: rgba(255,255,255,.86);
      padding: .55rem .9rem;
      cursor: pointer;
      display:inline-flex;
      align-items:center;
      gap:.55rem;
      box-shadow: 0 10px 24px rgba(2,6,23,.10);
      transition: transform .18s ease, box-shadow .18s ease;
    }
    .copy-bibtex-btn:hover{
      transform: translateY(-1px);
      box-shadow: 0 16px 36px rgba(2,6,23,.14);
    }

    /* New: Scientific Data section */
    .data-hero{
      background: radial-gradient(900px 500px at 10% -10%, rgba(6,182,212,.16), transparent 55%),
                  radial-gradient(900px 500px at 95% 0%, rgba(124,58,237,.14), transparent 55%),
                  linear-gradient(180deg, rgba(255,255,255,.92), rgba(246,248,252,.72));
      border-top: 1px solid var(--line);
      border-bottom: 1px solid var(--line);
    }
    .data-kpis{
      display:grid;
      grid-template-columns: repeat(12, 1fr);
      gap: 1rem;
      margin-top: 1.1rem;
    }
    .kpi{
      grid-column: span 4;
      background: rgba(255,255,255,.88);
      border: 1px solid var(--line);
      border-radius: var(--radius-2);
      box-shadow: var(--shadow);
      padding: 1rem 1rem .95rem 1rem;
    }
    .kpi .label{
      font-size: .86rem;
      color: rgba(91,100,118,.95);
      font-weight: 700;
      letter-spacing: .02em;
      text-transform: uppercase;
      margin-bottom: .35rem;
    }
    .kpi .value{
      font-size: 1.75rem;
      font-weight: 800;
      letter-spacing: -0.02em;
      line-height: 1.05;
    }
    .kpi .hint{
      margin-top: .45rem;
      color: rgba(91,100,118,.92);
      font-size: .95rem;
      line-height: 1.35;
    }
    .data-grid{
      display:grid;
      grid-template-columns: repeat(12, 1fr);
      gap: 1rem;
      margin-top: 1rem;
    }
    .panel{
      grid-column: span 6;
      background: rgba(255,255,255,.88);
      border: 1px solid var(--line);
      border-radius: var(--radius-2);
      box-shadow: var(--shadow);
      padding: 1rem;
    }
    .panel h3{
      font-size: 1.05rem;
      font-weight: 800;
      letter-spacing: -0.01em;
      margin-bottom: .5rem;
    }
    .panel p{
      color: rgba(91,100,118,.95);
      margin-top: .25rem;
      margin-bottom: .75rem;
      font-size: .98rem;
    }
    .panel canvas{
      width: 100% !important;
      height: 320px !important;
    }
    .panel.wide{
      grid-column: span 12;
    }
    .table.is-striped tbody tr:nth-child(odd){
      background: rgba(37,99,235,.04);
    }

    @media (max-width: 1024px){
      .kpi{ grid-column: span 6; }
      .panel{ grid-column: span 12; }
      .panel canvas{ height: 300px !important; }
    }
    @media (max-width: 768px){
      .kpi{ grid-column: span 12; }
    }

    /* Make your existing image sections look like “figure cards” without changing HTML */
    section.hero.is-small .hero-body > .container > .columns.is-centered > .column.is-four-fifths{
      background: rgba(255,255,255,.88);
      border: 1px solid var(--line);
      border-radius: var(--radius-2);
      box-shadow: var(--shadow);
      padding: 1.15rem;
    }
  </style>
</head>

<body>

  <!-- Sticky academic nav (adds polish; does not modify existing content blocks) -->
  <div class="academic-nav">
    <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="container is-max-desktop">
        <div class="navbar-brand">
          <a class="navbar-item" href="#main-content">
            <img src="static/images/logo.png" alt="Innovator-VL" style="height: 28px; width: auto;">
            <span style="font-weight:800; letter-spacing:-0.02em;">Innovator-VL</span>
            <span class="badge"><i class="ai ai-arxiv"></i> Technical Report</span>
          </a>

          <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navMenu">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
          </a>
        </div>

        <div id="navMenu" class="navbar-menu">
          <div class="navbar-end">
            <a class="navbar-item" href="#Abstract">Abstract</a>
            <a class="navbar-item" href="#Overall">Overall</a>
            <a class="navbar-item" href="#Performance">Performance</a>
            <a class="navbar-item" href="#TokenEfficiency">Token Efficiency</a>
            <a class="navbar-item" href="#TrainingData">Training Data</a>
            <a class="navbar-item" href="#DataPipeline">Pipeline</a>
            <a class="navbar-item" href="#ScientificData">Scientific Data</a>
            <a class="navbar-item" href="#BibTeX">BibTeX</a>
          </div>
        </div>
      </div>
    </nav>
  </div>

  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title"
                style="display:flex; align-items:center; justify-content:center; gap:0.2rem;">
              <img
                src="static/images/logo.png"
                alt="Innovator-VL logo"
                style="height: 64px; width: auto; display:block; transform: translate(180%, -50px);"
              >
              <span>Innovator-VL: A Multimodal Large Language Model for Scientific Discovery</span>
            </h1>

            <!-- <div class="is-size-5 publication-authors"> -->
              <!-- TODO: Replace with your paper authors and their personal links -->
            <div class="is-size-5 publication-authors">
              <span class="author-block">Zichen Wen</a><sup>*¹</sup>,</span>
              <span class="author-block">Boxue Yang</a><sup>*¹</sup>,</span>
              <span class="author-block">Shuang Chen</a><sup>¹</sup>,</span>
              <span class="author-block">Yaojie Zhang</a><sup>¹</sup>,</span>
              <span class="author-block">Yuhang Han</a><sup>¹</sup>,</span>
              <span class="author-block">Junlong Ke</a><sup>¹</sup>,</span>
              <span class="author-block">Cong Wang<sup>¹</sup>,</span>
              <span class="author-block">Yicheng Fu<sup>¹</sup>,</span>
              <span class="author-block">Jiawang Zhao<sup>¹</sup>,</span>
              <span class="author-block">Jiangchao Yao<sup>¹</sup>,</span>
              <span class="author-block">Xi Fang<sup>²</sup>,</span>
              <span class="author-block">Zhen Wang<sup>²</sup>,</span>
              <span class="author-block">Henxing Cai<sup>²</sup>,</span>
              <span class="author-block">Lin Yao<sup>²</sup>,</span>
              <span class="author-block">Zhifeng Gao<sup>²</sup>,</span>
              <span class="author-block">Yanhui Hong<sup>²</sup>,</span>
              <span class="author-block">Nang Yuan<sup>²</sup>,</span>
              <span class="author-block">Yixuan Li<sup>²</sup>,</span>
              <span class="author-block">Guojiang Zhao<sup>²</sup>,</span>
              <span class="author-block">Haoyi Tao<sup>²</sup>,</span>
              <span class="author-block">Nan Wang<sup>²</sup>,</span>
              <span class="author-block">Han Lyu<sup>²</sup>,</span>
              <span class="author-block">Guolin Ke<sup>²</sup>,</span>
              <span class="author-block">Ning Liao<sup>³</sup>,</span>
              <span class="author-block">Xiaoxing Wang<sup>³</sup>,</span>
              <span class="author-block">Kai Chen<sup>³</sup>,</span>
              <span class="author-block">Zhiyu Li<sup>³</sup>,</span>
              <span class="author-block">Feiyu Xiong<sup>³</sup>,</span>
              <span class="author-block">Sihan Hu<sup>⁴</sup>,</span>
              <span class="author-block">Kun Chen<sup>⁴</sup>,</span>
              <span class="author-block">Yanfeng Wang<sup>¹</sup>,</span>
              <span class="author-block">Weinan E<sup>¹†</sup>,</span>
              <span class="author-block">Linfeng Zhang<sup>²†</sup></span>
              <span class="author-block">Linfeng Zhang<sup>¹†</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>1</sup> School of Artificial Intelligence, Shanghai Jiao Tong University
                <br>
                <sup>2</sup> DP Technology
                <sup>3</sup> MemTensor
                <sup>4</sup> Institute of Theoretical Physics, Chinese Academy of Sciences
              </span>
              <span class="eql-cntrb"><small><br><sup>*</sup> Indicates Equal Contribution</small></span>
              <span class="eql-cntrb"><small><sup>†</sup> Indicates Corresponding Author</small></span>
            </div>


                <div class="column has-text-centered">
                  <div class="publication-links">

                    <!-- arXiv -->
                    <span class="link-block">
                      <a href="https://arxiv.org/abs/<ARXIV_PAPER_ID>" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span>
                    <!-- GitHub -->
                    <span class="link-block">
                      <a href="https://github.com/InnovatorLM/Innovator-VL" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fab fa-github"></i>
                        </span>
                        <span>Code</span>
                      </a>
                    </span>

                    <!-- HuggingFace Instruct Model -->
                    <span class="link-block">
                      <a href="https://huggingface.co/InnovatorLab/Innovator-VL-8B-Instruct" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-robot"></i>
                        </span>
                        <span>Instruct Model</span>
                      </a>
                    </span>

                    <!-- HuggingFace Thinking Model -->
                    <span class="link-block">
                      <a href="https://huggingface.co/InnovatorLab/Innovator-VL-8B-Thinking" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-brain"></i>
                        </span>
                        <span>Thinking Model</span>
                      </a>
                    </span>

                    <!-- HuggingFace Instruct Data -->
                    <span class="link-block">
                      <a href="https://huggingface.co/datasets/InnovatorLab/Innovator-VL-Instruct-46M" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-database"></i>
                        </span>
                        <span>Instruct Data</span>
                      </a>
                    </span>

                    <!-- HuggingFace RL Data -->
                    <span class="link-block">
                      <a href="https://huggingface.co/datasets/InnovatorLab/Innovator-VL-RL-172K" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-database"></i>
                        </span>
                        <span>RL Data</span>
                      </a>
                    </span>
                                        <!-- HuggingFace RL Data -->
                    <span class="link-block">
                      <a href="https://huggingface.co/datasets/InnovatorLab/OpenRxn" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-chart-line"></i>
                        </span>
                        <span>Benchmark: OpenRxn</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="https://huggingface.co/datasets/InnovatorLab/EMVista" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-trophy"></i>
                        </span>
                        <span>Benchmark: EMVista</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="https://huggingface.co/datasets/InnovatorLab/MolParse" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-chart-bar"></i>
                        </span>
                        <span>Benchmark: MolParse</span>
                      </a>
                    </span>


            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser image-->
<!-- Teaser image -->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img
        src="static/images/overall.png"
        alt="Overall illustration of Innovator-VL"
        style="display: block; margin: auto; max-width: 100%; height: auto;"
      >

      <h2 class="subtitle has-text-centered">
        Overall illustration of Innovator-VL.
      </h2>
    </div>
  </div>
</section> -->

<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light" id="Abstract">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
             We present Innovator-VL, a scientific multimodal large language model (MLLM) designed to advance multimodal understanding and reasoning across diverse scientific domains while still maintaining excellent performance on general vision tasks. Contrary to the prevailing trend of relying on massive domain-specific pretraining data and opaque training pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific multimodal intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent and end-to-end reproducible training pipeline for scientific multimodal modeling, covering all stages from data collection and cleaning to preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, together with detailed optimization and hyperparameter recipes. This enables faithful reproduction of our results and facilitates systematic extension and adaptation by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on a wide range of scientific tasks using fewer than five million carefully curated scientific training samples, despite not relying on large-scale scientific pretraining. These results highlight that effective scientific multimodal reasoning can be achieved through principled data selection and training strategies rather than indiscriminate data scaling. (iii) Third, Innovator-VL demonstrates strong generalization beyond scientific domains, achieving competitive performance among MLLMs of comparable size on general vision benchmarks, multimodal reasoning benchmarks, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified multimodal model without compromising general-purpose capabilities. Together, our practices suggest that, in the absence of large-scale scientific data, efficient, reproducible, and high-performing scientific multimodal models can be built, thereby providing a practical and transparent foundation for future research in scientific multimodal modeling.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image section (static) -->
<section class="hero is-small" id="Overall">
  <div class="hero-body">
    <div class="container">

      <!-- Title: centered -->
      <h2 class="title is-3 has-text-centered">Overall Illustration</h2>

      <div class="columns is-centered">
        <div class="column is-four-fifths has-text-centered">

          <!-- Image -->
          <img
            src="static/images/architecture_01.png"
            alt="Overall illustration of Innovator-VL"
            style="max-width: 100%; height: auto;"
            loading="lazy"
          >

          <!-- Figure caption -->
          <p class="has-text-grey is-size-6">
            Figure 1: Overall illustration of Innovator-VL.
          </p>

          <!-- Description (keep LEFT for readability) -->
          <ul class="content has-text-left" style="margin-top: 1rem;">
            <li>
              <strong>Scientific MLLM:</strong>
              A unified multimodal large language model for scientific understanding and reasoning.
            </li>
            <li>
              <strong>General + Scientific:</strong>
              Strong general vision–language capability without sacrificing scientific specialization.
            </li>
            <li>
              <strong>Data-Efficient Training:</strong>
              Competitive scientific performance achieved with carefully curated data instead of large-scale domain-specific pretraining.
            </li>
            <li>
              <strong>Transparent Pipeline:</strong>
              End-to-end reproducible training covering data curation, instruction tuning, and reinforcement learning.
            </li>
            <li>
              <strong>Strong Generalization:</strong>
              Consistent performance across general vision, multimodal reasoning, and diverse scientific benchmarks.
            </li>
          </ul>

        </div>
      </div>
    </div>
  </div>
</section>




<!-- End image carousel -->

<section class="hero is-small" id="Performance">
  <div class="hero-body">
    <div class="container">

      <!-- Title: centered -->
      <h2 class="title is-3 has-text-centered">Performance</h2>

      <div class="columns is-centered">
        <div class="column is-four-fifths has-text-centered">

          <!-- Image -->
          <img
            src="static/images/bar_figure_01.png"
            alt="Performance of Innovator-VL"
            style="max-width: 100%; height: auto;"
            loading="lazy"
          >

          <!-- Figure caption -->
          <p class="has-text-grey is-size-6">
            Figure 2: Performance of Innovator-VL across general, reasoning, and scientific benchmarks.
          </p>

          <!-- Description (keep LEFT, more academic) -->
          <p class="content has-text-left" style="margin-top: 1rem;">
            <strong>General:</strong> Innovator-VL-8B-Instruct achieves top-tier performance on general multimodal benchmarks, matching or surpassing leading open-source models in visual perception, OCR, document understanding, and real-world reasoning.
            <br><br>
            <strong>Math &amp; Reasoning:</strong> Innovator-VL-8B-Thinking further improves multimodal reasoning, attaining the highest average scores on visual math and reasoning benchmarks among comparable 8B models through reinforcement learning–enhanced long-horizon reasoning.
            <br><br>
            <strong>Science:</strong> Innovator-VL consistently outperforms general-purpose baselines, delivering strong results on chemistry, reaction understanding, molecular parsing, microscopy analysis, and scientific VQA tasks.
          </p>

        </div>
      </div>

    </div>
  </div>
</section>


<!-- End youtube video -->
<!-- ====================================================================== -->
<!-- ================== SECTION: TOKEN EFFICIENCY OF REASONING ============== -->
<!-- ====================================================================== -->
 <section class="hero is-small is-centered" id="TokenEfficiency">

<!-- <section class="section hero is-centered" id="TokenEfficiency"> -->
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">

        <h2 class="title is-3 has-text-centered">On Token Efficiency of Reasoning</h2>

        <div class="content has-text-justified">
          <p>
            Innovator-VL exhibits strong efficiency in reasoning token consumption. As shown in Figure&nbsp;5,
            Innovator-VL-8B-Thinking achieves competitive or superior accuracy on mathematical reasoning benchmarks
            while using substantially fewer tokens than comparable baselines, leading to lower inference cost
            and reduced latency.
          </p>

          <p>
            This efficiency is primarily driven by the reinforcement learning stage, which encourages the model
            to focus on critical reasoning steps and eliminate redundant computation, resulting in more compact
            and effective reasoning trajectories. Such token-efficient reasoning is particularly beneficial for
            scientific applications involving long-context and multi-step reasoning.
          </p>
        </div>


        <!-- Figure -->
        <div class="has-text-centered" style="margin-top: 1.25rem;">
          <!-- 请把图片放到这个路径（或改成你的实际路径） -->
          <img
            src="static/images/token_efficiency_01.png"
            alt="Accuracy and token efficiency comparison on mathematical reasoning benchmarks"
            style="max-width: 100%; height: auto;"
            loading="lazy"
          >
          <p class="has-text-grey is-size-6" style="margin-top: 0.5rem;">
            Figure 3: Accuracy and token efficiency comparison on mathematical reasoning benchmarks.
            Innovator-VL-8B-Thinking achieves the highest accuracy while consuming the fewest tokens across benchmarks.
          </p>
        </div>

      </div>
    </div>
  </div>
</section>


<!-- Training Data -->
<section class="hero is-small" id="TrainingData">
  <div class="hero-body">
    <div class="container">

      <!-- Title: centered -->
      <h2 class="title is-3 has-text-centered">Training Data</h2>

      <div class="columns is-centered">
        <div class="column is-four-fifths has-text-centered">

          <!-- Image -->
          <img
            src="static/images/pie_figure_01.png"
            alt="Data distribution across different training stages of Innovator-VL"
            style="max-width: 100%; height: auto;"
            loading="lazy"
          >

          <!-- Figure caption -->
          <p class="has-text-grey is-size-6">
            Figure 4: Data distribution across different training stages of Innovator-VL.
            (a) Innovator-VL-Mid-Training-85M,
            (b) Innovator-VL-Instruct-46M,
            (c) Innovator-VL-RL-172K.
          </p>

          <!-- Description -->
          <p class="content has-text-left" style="margin-top: 1rem;">
            <strong>Mid-Training:</strong>
            The mid-training stage focuses on large-scale multimodal alignment, combining diverse general-domain
            and scientific data to establish robust visual perception and cross-modal representation learning.
            <br><br>

            <strong>Instruction Tuning:</strong>
            The instruction-tuning stage emphasizes high-quality supervised data, integrating general
            instruction-following samples with carefully curated scientific tasks to enhance controllability
            and domain-specific understanding.
            <br><br>

            <strong>Reinforcement Learning:</strong>
            The reinforcement learning stage uses a compact but targeted dataset to further refine long-horizon
            reasoning, response consistency, and decision-making quality.
          </p>

        </div>
      </div>

    </div>
  </div>
</section>

<!-- ====================================================================== -->
<!-- ================== DATA CONSTRUCTION PIPELINE ========================= -->
<!-- ====================================================================== -->
<section class="hero is-small is-light" id="DataPipeline">
  <div class="hero-body">
    <div class="container">

      <!-- Title: centered -->
      <h2 class="title is-3 has-text-centered">Data Construction Pipeline</h2>

      <div class="columns is-centered">
        <div class="column is-four-fifths has-text-centered">

          <!-- Image -->
          <img
            src="static/images/data_pipeline_01.png"
            alt="Data construction pipeline of Innovator-VL"
            style="max-width: 80%; height: auto;"
            loading="lazy"
          >

          <!-- Figure caption -->
          <p class="has-text-grey is-size-6">
            Figure 5: Overview of data construction pipelines for scientific multimodal training.
          </p>

          <!-- Description (keep LEFT for readability) -->
          <p class="content has-text-left" style="margin-top: 1rem;">
            <strong>Optical Chemical Structure Recognition (OCSR):</strong>
            A human-in-the-loop pipeline combines large-scale synthetic bootstrapping with active-learning-driven expansion on real patent and paper data, using E-SMILES as a unified annotation format to represent complex chemical structures.
            <br><br>
            <strong>Chemical Reaction Understanding:</strong>
            Reaction datasets are constructed from scientific PDFs via automated layout parsing and expert-verified question generation, covering both fine-grained reaction perception and document-level multimodal reasoning.
            <br><br>
            <strong>Electron Microscopy (EM) Microstructures:</strong>
            Large-scale EM datasets are built through iterative expert annotation and model-assisted refinement, with dense instance-level segmentation and structured attribute descriptions to support microstructural analysis.
          </p>

        </div>
      </div>

    </div>
  </div>
</section>

<!-- ====================================================================== -->
<!-- ================== NEW SECTION: SCIENTIFIC DATA OVERVIEW =============== -->
<!-- ====================================================================== -->
<section class="section hero" id="ScientificData">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Scientific Data Overview</h2>
        <p class="subtitle is-6 has-text-grey">
          Curated multimodal scientific data supporting reproducible and data-efficient training
        </p>
      </div>
    </div>

    <!-- ================== DATA CARDS ================== -->
    <div class="columns is-centered">
      <div class="column is-3">
        <div class="box has-text-centered">
          <p class="heading">Total Scientific Samples</p>
          <p class="title is-4">≈ 4.8M</p>
        </div>
      </div>
      <div class="column is-3">
        <div class="box has-text-centered">
          <p class="heading">Scientific Domains</p>
          <p class="title is-4">6+</p>
        </div>
      </div>
      <div class="column is-3">
        <div class="box has-text-centered">
          <p class="heading">Modalities</p>
          <p class="title is-4">Image · Text · Structure</p>
        </div>
      </div>
    </div>

    <!-- ================== DOMAIN BREAKDOWN ================== -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">Domain-wise Composition</h3>

        <table class="table is-fullwidth is-striped is-hoverable">
          <thead>
            <tr>
              <th>Scientific Domain</th>
              <th>Primary Tasks</th>
              <th>Modalities</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Chemistry</td>
              <td>OCSR, reaction understanding, molecule parsing</td>
              <td>Image + Structured Text</td>
            </tr>
            <tr>
              <td>Materials Science</td>
              <td>Microstructure analysis, EM interpretation</td>
              <td>Microscopy Image + Text</td>
            </tr>
            <tr>
              <td>Scientific Documents</td>
              <td>Figure understanding, document-level reasoning</td>
              <td>PDF Layout + Image + Text</td>
            </tr>
            <tr>
              <td>General Science QA</td>
              <td>Visual question answering, reasoning</td>
              <td>Image + Natural Language</td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>

    <!-- ================== TRAINING STAGE DATA ================== -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">Training Stage Statistics</h3>

        <table class="table is-fullwidth is-bordered">
          <thead>
            <tr>
              <th>Stage</th>
              <th>Dataset</th>
              <th>Scale</th>
              <th>Purpose</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Mid-Training</td>
              <td>Innovator-VL-Mid-Training</td>
              <td>85M samples</td>
              <td>Multimodal alignment and representation learning</td>
            </tr>
            <tr>
              <td>Instruction Tuning</td>
              <td>Innovator-VL-Instruct</td>
              <td>46M samples</td>
              <td>Instruction following and scientific controllability</td>
            </tr>
            <tr>
              <td>Reinforcement Learning</td>
              <td>Innovator-VL-RL</td>
              <td>172K trajectories</td>
              <td>Long-horizon reasoning and decision refinement</td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>
    <!-- ====================================================================== -->
<!-- ================== NEW SECTION: SCIENTIFIC DATA GALLERY (CAROUSEL) ===== -->
<!-- ====================================================================== -->
<!-- ====================================================================== -->
<!-- ================== SCIENTIFIC DATA GALLERY ============================ -->
<!-- ====================================================================== -->
<section class="section hero is-light" id="scientific-gallery">
  <div class="container is-max-desktop">

    <!-- Title -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Scientific Data Gallery</h2>
      </div>
    </div>

    <!-- Carousel -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">

        <!-- IMPORTANT: unique id to avoid being overridden by other carousels -->
        <div id="scientificCarousel"
             class="carousel scientific-carousel"
             data-autoplay="false"
             data-loop="true">

          <div class="item">
            <img src="static/images/reaction1.png" alt="Reaction sample 1">
          </div>

          <div class="item">
            <img src="static/images/reaction2.png" alt="Reaction sample 2">
          </div>

          <div class="item">
            <img src="static/images/reaction3.png" alt="Reaction sample 3">
          </div>

          <div class="item">
            <img src="static/images/reaction4.png" alt="Reaction sample 4">
          </div>

          <div class="item">
            <img src="static/images/reaction5.png" alt="Reaction sample 5">
          </div>

          <div class="item">
            <img src="static/images/reaction6.png" alt="Reaction sample 6">
          </div>

        </div>

      </div>
    </div>
  </div>
</section>




    <!-- ================== DATA QUALITY ================== -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">Data Quality & Curation Principles</h3>

        <div class="content has-text-justified">
          <ul>
            <li>
              <strong>Expert-in-the-loop Verification:</strong>
              Scientific samples are iteratively reviewed and refined with domain experts to ensure correctness and semantic fidelity.
            </li>
            <li>
              <strong>Structure-aware Annotation:</strong>
              Chemical structures and reactions are represented using E-SMILES and structured descriptions to preserve scientific validity.
            </li>
            <li>
              <strong>Minimal Redundancy, Maximal Coverage:</strong>
              Data selection prioritizes diversity and task coverage over raw scale, improving data efficiency and generalization.
            </li>
            <li>
              <strong>Reproducibility-first Design:</strong>
              All datasets, preprocessing steps, and training configurations are explicitly documented and publicly released.
            </li>
          </ul>
        </div>
      </div>
    </div>

  </div>
</section>

<!-- ====================================================================== -->
<!-- ================== END OF NEW SCIENTIFIC DATA SECTION ================= -->
<!-- ====================================================================== -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">

    <div class="bibtex-header"
         style="display:flex; align-items:center; justify-content:space-between; gap:1rem;">
      <h2 class="title is-3">BibTeX</h2>

      <!-- Copy button -->
      <button
        class="button is-small is-info is-light"
        onclick="copyBibTeX()"
        title="Copy BibTeX to clipboard"
      >
        <span class="icon is-small">
          <i class="fas fa-copy"></i>
        </span>
        <span>Copy</span>
      </button>
    </div>

<pre id="bibtex-code"><code>@article{innovator_vl_2024,
  title   = {Innovator-VL: A Multimodal Large Language Model for Scientific Discovery},
  author  = {Wen, Zichen and Yang, Boxue and Chen, Shuang and Zhang, Yaojie and Han, Yuhang and Ke, Junlong and Zhang, Linfeng},
  journal = {Technical Report},
  year    = {2024},
  url     = {https://innovator-vl.github.io/}
}</code></pre>

  </div>
</section>

<!--End BibTex citation -->

  <!-- ================== 原 BibTeX / Footer / Tracking 等内容继续保持 ================== -->

</body>
</html>